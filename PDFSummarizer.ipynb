{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1iOc500sY0z"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf gradio transformers pinecone groq sentence_transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymupdf\n",
        "doc = pymupdf.open('pizza_description.pdf')\n",
        "\n",
        "doc_text = \"\"\n",
        "for page_num in range(doc.page_count):\n",
        "    page = doc[page_num]\n",
        "    doc_text += page.get_text()\n",
        "\n",
        "doc.close()\n",
        "\n",
        "print(doc_text)"
      ],
      "metadata": {
        "id": "17OAVdi_vd00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size=50\n",
        "overlap=10\n",
        "\n",
        "# chunk_size (int): The number of words in each chunk.\n",
        "# overlap (int): The number of overlapping words between consecutive chunks.\n",
        "\n",
        "words = doc_text.split()\n",
        "chunks = []\n",
        "\n",
        "begin = 0\n",
        "while begin < len(words):\n",
        "    end = begin + chunk_size\n",
        "    # Extract the chunk and join it back into a string\n",
        "    chunk = ' '.join(words[begin:end])\n",
        "    chunks.append(chunk)\n",
        "\n",
        "    # Move to the next chunk, starting from 'chunk_size - overlap' words ahead\n",
        "    begin += chunk_size - overlap\n",
        "\n",
        "# Example usage\n",
        "print(len(chunks))\n",
        "print(chunks)"
      ],
      "metadata": {
        "id": "6N-Xs6Q6uCV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2') #supposed to be fast\n",
        "# model = SentenceTransformer('deberta-v3-base') #supposed to be pretty good\n",
        "model = SentenceTransformer('roberta-base')\n",
        "\n",
        "embeddings = [model.encode(chunk) for chunk in chunks]\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "id": "WpXATJhe__wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=\"\")\n",
        "\n",
        "index_name = \"my-index\"\n"
      ],
      "metadata": {
        "id": "uWOrZCQIKgah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc.delete_index(index_name)"
      ],
      "metadata": {
        "id": "pOnkMM1bely2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not pc.has_index(index_name):\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        # dimension=384,\n",
        "        dimension=768, #for roberta\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'\n",
        "        )\n",
        "    )\n",
        "\n",
        "while not pc.describe_index(index_name).status['ready']:\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "AzXBWnQDehGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pc.Index(index_name)\n",
        "\n",
        "records = []\n",
        "for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
        "    records.append({\n",
        "        \"id\": f\"chunk_{i}\",  # Unique ID for each chunk\n",
        "        \"values\": embedding.tolist(),  # Convert embedding to list\n",
        "        \"metadata\": {\"text\": chunk}    # Attach text chunk as metadata\n",
        "    })\n",
        "\n",
        "index.upsert(vectors=records, namespace=\"example-namespace\")\n",
        "print(\"Embeddings uploaded successfully.\")"
      ],
      "metadata": {
        "id": "hj3rWsWodUtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import numpy as np\n",
        "\n",
        "groq_api_key = \"\"\n",
        "groq_client = Groq(api_key=groq_api_key)"
      ],
      "metadata": {
        "id": "t26DabetKf52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response_from_groq(context, query):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are a helpful assistant and will answer questions based on the following context:\\n\\n{context}\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": query\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    response = groq_client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=\"llama3-70b-8192\"\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "hwpR_jd7oC1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_and_answer(query, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieve relevant chunks and generate an answer using GROQ\n",
        "\n",
        "    Args:\n",
        "        query (str): User's question\n",
        "        top_k (int): Number of relevant chunks to retrieve\n",
        "    \"\"\"\n",
        "    # Create embedding for the query\n",
        "    query_embedding = model.encode(query)\n",
        "\n",
        "    # Search in Pinecone\n",
        "    search_results = index.query(\n",
        "        vector=query_embedding.tolist(),\n",
        "        top_k=top_k,\n",
        "        namespace=\"example-namespace\",\n",
        "        include_metadata=True\n",
        "    )\n",
        "\n",
        "    relevant_chunks = [match.metadata[\"text\"] for match in search_results.matches]\n",
        "    context = \"\\n\".join(relevant_chunks)\n",
        "\n",
        "    answer = get_response_from_groq(context, query)\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"relevant_chunks\": relevant_chunks,\n",
        "        \"context\": context\n",
        "    }"
      ],
      "metadata": {
        "id": "5WkXWlTif0cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the document about?\"\n",
        "result = retrieve_and_answer(query)\n",
        "\n",
        "print(\"Answer:\", result[\"answer\"])\n",
        "print(\"\\nRelevant chunks used:\")\n",
        "for i, chunk in enumerate(result[\"relevant_chunks\"], 1):\n",
        "    print(f\"\\nChunk {i}:\")\n",
        "    print(chunk)"
      ],
      "metadata": {
        "id": "sLHmzvuZi8y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_interface(query):\n",
        "    result = retrieve_and_answer(query)\n",
        "    return result[\"answer\"], \"\\n\\n\".join(result[\"relevant_chunks\"])\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Textbox(label=\"Ask a question about the document\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Answer\"),\n",
        "        gr.Textbox(label=\"Retrieved Chunks\")\n",
        "    ],\n",
        "    title=\"Document Q&A\",\n",
        "    description=\"Ask questions about the uploaded document and get AI-powered answers.\"\n",
        ")\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "1dczVAeQi8h8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}